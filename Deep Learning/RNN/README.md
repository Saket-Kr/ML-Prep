In normal Vanilla RNNs, gradiants during back propogation start vanishing to a large extent, losing context information when the sentence is really long. Layers that get a small gradient update stop learning. Those are usually the earlier layers. So because these layers don’t learn, RNN’s can forget what it has seen in longer sequences, thus having a short-term memory.

We lose all the information related to the local spatiality of the images. In particular, this piece of code transforms the bitmap, representing each written digit into a flat vector where the spatial locality is gone. Convolutional neural networks (CNN) takes into account both the idea of preserving the spatial locality in images (and, more generally, in any type of information) and the idea of learning via progressive levels of abstraction: with one layer, you can only learn simple patterns; with more than one layer, you can learn multiple patterns. 
