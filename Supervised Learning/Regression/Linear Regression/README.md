**Linear Regression**

- Among various regression methods available, Linear regression is one. Here we try to find a function that maps some features or variables to others sufficiently well.

- When implementing linear regression of some dependent variable ğ‘¦ on the set of independent variables ğ± = (ğ‘¥â‚, â€¦, ğ‘¥áµ£), where ğ‘Ÿ is the number of predictors, you assume a linear relationship between ğ‘¦ and ğ±: ğ‘¦ = ğ›½â‚€ + ğ›½â‚ğ‘¥â‚ + â‹¯ + ğ›½áµ£ğ‘¥áµ£ + ğœ€. This equation is the regression equation. ğ›½â‚€, ğ›½â‚, â€¦, ğ›½áµ£ are the regression coefficients, and ğœ€ is the random error.

- Linear regression calculates the estimators of the regression coefficients or simply the predicted weights, denoted with ğ‘â‚€, ğ‘â‚, â€¦, ğ‘áµ£. They define the estimated regression function ğ‘“(ğ±) = ğ‘â‚€ + ğ‘â‚ğ‘¥â‚ + â‹¯ + ğ‘áµ£ğ‘¥áµ£.

- The differences ğ‘¦áµ¢ - ğ‘“(ğ±áµ¢) for all observations ğ‘– = 1, â€¦, ğ‘›, are called the residuals.

- To get the best weights, you usually minimize the sum of squared residuals (SSR) for all observations ğ‘– = 1, â€¦, ğ‘›: SSR = Î£áµ¢(ğ‘¦áµ¢ - ğ‘“(ğ±áµ¢))Â². This approach is called the method of ordinary least squares.

- The coefficient of determination, denoted as ğ‘…Â², tells you which amount of variation in ğ‘¦ can be explained by the dependence on ğ± using the particular regression model. Larger ğ‘…Â² indicates a better fit and means that the model can better explain the variation of the output with different inputs.

- The value ğ‘…Â² = 1 corresponds to SSR = 0, that is to the perfect fit since the values of predicted and actual responses fit completely to each other.

- *Asymptote:* Straight line that continually approaches a given curve but does not meet it at any finite distance. Sigmoid function becomes asymptote to y=1 for positive values of x and becomes asymptote to y=0 for negative values of x.

Concept of **slope** and **intercept**:  
The estimated regression function has the equation ğ‘“(ğ‘¥) = ğ‘â‚€ + ğ‘â‚ğ‘¥.
  - *Intercept*: The value of ğ‘â‚€, is also called the intercept. It shows the point where the estimated regression line crosses the ğ‘¦ axis. It is the value of the estimated response ğ‘“(ğ‘¥) for ğ‘¥ = 0. For example, in the equation ğ‘“(ğ‘¥) *= 140 + 4.3*ğ‘¥ The value of the y-intercept indicates the average job skill score for an employee with no training. The value of the slope (4.3) indicates that for each hour of training, the job skill score increases, on average, by 4.3 points.

  - *Slope*:  The value of ğ‘â‚ determines the slope of the estimated regression line. It indicates the steepness of a line and the intercept indicates the location where it intersects an axis, say y-axis. For example, a company determines that job performance for employees in a production department can be predicted using the regression model y = 130 + 4.3x, where x is the hours of in-house training they receive (from 0 to 20) and y is their score on a job skills test.

***The polynomial regression** problem can be solved as a linear problem with the term ğ‘¥Â² regarded as an input variable.*
